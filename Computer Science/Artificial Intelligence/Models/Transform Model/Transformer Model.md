
# About

It is a [[Deep Learning]] Architecture developed by [[Google]]
Used in [[Generative Pre-trained Transformer]]

Created by Google
- [Attention is All You Need - Wiki](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need) 
- [[Google - Attention is All you need.pdf]]

It uses parallelism to speed up training.

# References

- [Wiki Page about Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))
- **Andrej Karpathy**
	- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
	- [Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy](https://www.youtube.com/watch?v=XfpMkf4rD6E)
	- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
	- [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)
- **3Blue1Brown**
	- [But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning](https://www.youtube.com/watch?v=wjZofJX0v4M)
	- [Visualizing Attention, the Heart of a Transformer | Chapter 6, Deep Learning](https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7)