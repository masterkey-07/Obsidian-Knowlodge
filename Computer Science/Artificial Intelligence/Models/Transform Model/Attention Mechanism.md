It is to draw global dependencies between input and output, to prevent from what [[Recurrent Neural Networks]] do. 

**Types of [[Attention Mechanism]]**

- [[Self-Attention Mechanism]]

>[!question] What is Self-Attention (or Intra Attention)
>