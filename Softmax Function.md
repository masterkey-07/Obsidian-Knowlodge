TheÂ **softmax function,**Â also known asÂ **softargmax** orÂ **normalized exponential function**,â€ŠÂ converts a vector ofÂ KÂ real numbers into aÂ [[Probability Distribution]]Â ofÂ KÂ possible outcomes.

It is a generalization of the [[Logistic Function]] to multiple dimensions, and used inÂ multinomial logistic regression. The softmax function is often used as the lastÂ activation functionÂ of aÂ [[Neural Network]]Â to normalize the output of a network to aÂ [[Probability Distribution]]Â over predicted output classes.

# Definition

The softmax function takes as input a vectorÂ $z$Â ofÂ $K$Â real numbers, and normalizes it into aÂ [[Probability Distribution]]Â consisting ofÂ $K$Â probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in theÂ intervalÂ ${\displaystyle (0,1)}$, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.

Formally, the standard (unit) softmax functionÂ  $\sigma:\mathbb{R}^K \to (0,1)^K$, whereÂ ${\displaystyle K\geq 1}$, takes a vectorÂ ${\displaystyle \mathbf {z} =(z*{1},\dotsc ,z*{K})\in \mathbb {R} ^{K}}$Â and computes each component of vectorÂ ${\displaystyle \sigma (\mathbf {z} )\in (0,1)^{K}}$Â with

$$\sigma(z)_i = \frac{e^{z_i}}{\sum^K_{j=1}e^{z_j}}$$

In words, the softmax applies the standardÂ exponential functionÂ to each elementÂ ğ‘§ğ‘–!{\displaystyle z\_{i}}Â of the input vectorÂ ğ‘§!{\displaystyle \mathbf {z} }Â (consisting ofÂ ğ¾!{\displaystyle K}Â real numbers), and normalizes these values by dividing by the sum of all these exponentials. The normalization ensures that the sum of the components of the output vectorÂ ğœ(ğ‘§)!{\displaystyle \sigma (\mathbf {z} )}Â is 1. The term "softmax" derives from the amplifying effects of the exponential on any maxima in the input vector. For example, the standard softmax ofÂ (1,2,8)!{\displaystyle (1,2,8)}Â is approximatelyÂ (0.001,0.002,0.997)!{\displaystyle (0.001,0.002,0.997)}, which amounts to assigning almost all of the total unit weight in the result to the position of the vector's maximal element (of 8).

In general, instead ofÂ eÂ a differentÂ base "Base (exponentiation)")Â b > 0Â can be used. As above, ifÂ b > 1Â then larger input components will result in larger output probabilities, and increasing the value ofÂ bÂ will create probability distributions that are more concentrated around the positions of the largest input values. Conversely, ifÂ 0 < b < 1Â then smaller input components will result in larger output probabilities, and decreasing the value ofÂ bÂ will create probability distributions that are more concentrated around the positions of the smallest input values. WritingÂ ğ‘=ğ‘’ğ›½!{\displaystyle b=e^{\beta }}Â orÂ ğ‘=ğ‘’âˆ’ğ›½!{\displaystyle b=e^{-\beta }}[a]Â (for realÂ Î²)[b]Â yields the expressions:[c]

ğœ(ğ‘§)ğ‘–=ğ‘’ğ›½ğ‘§ğ‘–âˆ‘ğ‘—=1ğ¾ğ‘’ğ›½ğ‘§ğ‘—Â orÂ ğœ(ğ‘§)ğ‘–=ğ‘’âˆ’ğ›½ğ‘§ğ‘–âˆ‘ğ‘—=1ğ¾ğ‘’âˆ’ğ›½ğ‘§ğ‘—Â forÂ ğ‘–=1,â€¦,ğ¾.

!{\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{\beta z_{i}}}{\sum _{j=1}^{K}e^{\beta z_{j}}}}{\text{ or }}\sigma (\mathbf {z} )_{i}={\frac {e^{-\beta z_{i}}}{\sum _{j=1}^{K}e^{-\beta z_{j}}}}{\text{ for }}i=1,\dotsc ,K.}

A value proportional to the reciprocal ofÂ Î²Â is sometimes referred to as theÂ *temperature*:Â ğ›½=1/ğ‘˜ğ‘‡!{\textstyle \beta =1/kT}, whereÂ kÂ is typically 1 or theÂ Boltzmann constantÂ andÂ TÂ is the temperature. A higher temperature results in a more uniform output distribution (i.e. with higher entropy; it is "more random"), while a lower temperature results in a sharper output distribution, with one value dominating.

In some fields, the base is fixed, corresponding to a fixed scale,[d]Â while in others the parameterÂ Î²Â (orÂ T) is varied.