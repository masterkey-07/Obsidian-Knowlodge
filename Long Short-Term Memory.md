There is a [[Extended  Long Short-Term Memory]]
Is a type of [[Recurrent Neural Networks]]


```ad-note
Title: Source: [Wifi - Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory)

**Long short-term memory** (**LSTM**)[[1]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-lstm1997-1) is a type of [[Recurrent Neural Networks]] (RNN) aimed at dealing with the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem "Vanishing gradient problem")[[2]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-hochreiter1991-2) present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, [hidden Markov models](https://en.wikipedia.org/wiki/Hidden_Markov_models "Hidden Markov models") and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "**long** short-term memory".[[1]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-lstm1997-1) It is applicable to [classification](https://en.wikipedia.org/wiki/Classification_in_machine_learning "Classification in machine learning"), [processing](https://en.wikipedia.org/wiki/Computer_data_processing "Computer data processing") and [predicting](https://en.wikipedia.org/wiki/Predict "Predict") data based on [time series](https://en.wikipedia.org/wiki/Time_series "Time series"), such as in [handwriting](https://en.wikipedia.org/wiki/Handwriting_recognition "Handwriting recognition"),[[3]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-graves2009-3) [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition "Speech recognition"),[[4]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-sak2014-4)[[5]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-liwu2015-5) [machine translation](https://en.wikipedia.org/wiki/Machine_translation "Machine translation"),[[6]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-GoogleTranslate-6)[[7]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-FacebookTranslate-7) speech activity detection,[[8]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-8) robot control,[[9]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-mayer2006-9)[[10]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-OpenAIhand-10) video games,[[11]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-OpenAIfive-11)[[12]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-alphastar-12) and healthcare.[[13]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-decade2022-13)

A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate**[[14]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-hochreiter1996-14) and a **forget gate**.[[15]](https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-lstm2000-15) The cell remembers values over arbitrary time intervals and the three _gates_ regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
```